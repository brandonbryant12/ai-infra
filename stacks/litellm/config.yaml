model_list:
# OpenRouter - Cloud model discovery (70+ models)
- model_name: openrouter/*
  litellm_params:
    model: openrouter/*
    api_key: os.environ/OPENROUTER_API_KEY

# Ollama - Local model discovery (uncomment to enable)
# - model_name: ollama/*
#   litellm_params:
#     model: ollama/*
#     api_base: http://localhost:11434  # Default Ollama port

# vLLM - High-performance model serving (uncomment to enable)
# - model_name: vllm/*
#   litellm_params:
#     model: vllm/*
#     api_base: http://localhost:8000  # Default vLLM port
#
# HOW WILDCARD MODEL DISCOVERY WORKS:
# 1. Pattern Matching: Wildcard patterns like "vllm/*" are converted to regex
#    - The * becomes (.*) to match any model name after the provider prefix
#    - Example: "vllm/*" matches "vllm/gpt-4", "vllm/llama-2", etc.
# 2. Model Discovery: When check_provider_endpoint: true is set (see below),
#    LiteLLM queries the provider's /v1/models endpoint to get available models
# 3. Open WebUI Integration: Open WebUI calls LiteLLM's /v1/models endpoint
#    which returns all discovered models in OpenAI-compatible format
# 4. Implementation Details:
#    - Pattern matching: https://github.com/BerriAI/litellm/blob/main/litellm/router_utils/pattern_match_deployments.py
#    - vLLM integration: https://github.com/BerriAI/litellm/blob/main/litellm/llms/vllm/common_utils.py
#    - Router logic: https://github.com/BerriAI/litellm/blob/main/litellm/router.py
#    - Requires provider to expose OpenAI-compatible /v1/models endpoint

# IMPORTANT: Model discovery with wildcard (*) ONLY works for providers 
# that LiteLLM has built-in discovery support for (openrouter, ollama, vllm, etc.)
#
# For custom OpenAI-compatible servers, you CANNOT use wildcards.
# You must manually define each model like this:
# - model_name: my-custom-model
#   litellm_params:
#     model: openai/my-custom-model  # openai/ prefix with custom api_base
#     api_base: http://my-server:8000  # Your custom server URL
#     api_key: optional-key
#
# The openai/* wildcard would try to discover from api.openai.com,
# NOT from your custom api_base, so it won't work for custom servers.

general_settings:
  user_header_name: x-openwebui-user-name 
  master_key: os.environ/LITELLM_MASTER_KEY
  health_check_interval: 300
  detailed_debug: true
litellm_settings:
  check_provider_endpoint: true
  drop_params: true
  set_verbose: false
  request_timeout: 600
  success_callback:
  - langfuse
  failure_callback:
  - langfuse
