# LiteLLM Proxy Configuration
# This is a comprehensive example showing various model configurations

model_list:
  # OpenWebUI Integration
  - model_name: openwebui-default
    litellm_params:
      model: openai/openwebui-model
      api_base: http://myinstance.com/api
      api_key: os.environ/OPENWEBUI_API_KEY  # Reference environment variable
      timeout: 600
      stream_timeout: 60
      
  - model_name: openwebui-gpt4
    litellm_params:
      model: openai/gpt-4
      api_base: http://myinstance.com/api
      api_key: os.environ/OPENWEBUI_API_KEY
      
  # OpenAI Direct
  - model_name: gpt-4
    litellm_params:
      model: gpt-4
      api_key: os.environ/OPENAI_API_KEY
      organization: os.environ/OPENAI_ORG_ID  # Optional
      
  - model_name: gpt-3.5-turbo
    litellm_params:
      model: gpt-3.5-turbo
      api_key: os.environ/OPENAI_API_KEY
      
  # Azure OpenAI
  - model_name: azure-gpt-4
    litellm_params:
      model: azure/gpt-4-deployment
      api_base: https://your-resource.openai.azure.com/
      api_key: os.environ/AZURE_API_KEY
      api_version: "2024-02-15-preview"
      
  # Anthropic Claude
  - model_name: claude-3-opus
    litellm_params:
      model: claude-3-opus-20240229
      api_key: os.environ/ANTHROPIC_API_KEY
      
  - model_name: claude-3-sonnet
    litellm_params:
      model: claude-3-sonnet-20240229
      api_key: os.environ/ANTHROPIC_API_KEY
      
  # Google Vertex AI
  - model_name: gemini-pro
    litellm_params:
      model: vertex_ai/gemini-pro
      vertex_project: your-gcp-project
      vertex_location: us-central1
      
  # AWS Bedrock
  - model_name: bedrock-claude
    litellm_params:
      model: bedrock/anthropic.claude-v2
      aws_access_key_id: os.environ/AWS_ACCESS_KEY_ID
      aws_secret_access_key: os.environ/AWS_SECRET_ACCESS_KEY
      aws_region_name: us-east-1
      
  # Cohere
  - model_name: command-xlarge
    litellm_params:
      model: command-xlarge-nightly
      api_key: os.environ/COHERE_API_KEY
      
  # Hugging Face
  - model_name: huggingface-llama
    litellm_params:
      model: huggingface/meta-llama/Llama-2-7b-chat-hf
      api_key: os.environ/HUGGINGFACE_API_KEY
      
  # Local Models (Ollama)
  - model_name: ollama-llama2
    litellm_params:
      model: ollama/llama2
      api_base: http://localhost:11434
      
  # Local Models (LM Studio)
  - model_name: lmstudio-model
    litellm_params:
      model: openai/local-model
      api_base: http://localhost:1234/v1
      api_key: dummy-key
      
  # vLLM Server
  - model_name: vllm-vicuna
    litellm_params:
      model: openai/vicuna-7b
      api_base: http://localhost:8000
      api_key: dummy-key
      
  # Custom OpenAI-compatible endpoint
  - model_name: custom-llm
    litellm_params:
      model: openai/custom-model
      api_base: https://your-custom-api.com/v1
      api_key: os.environ/CUSTOM_API_KEY
      custom_llm_provider: openai  # Specify provider

# Router Settings (Load Balancing)
router_settings:
  model_group_alias:
    gpt-4-turbo: ["gpt-4", "azure-gpt-4"]  # Fallback list
    claude-3: ["claude-3-opus", "claude-3-sonnet"]
    
  routing_strategy: "least-busy"  # Options: simple-shuffle, least-busy, usage-based-routing
  
  num_retries: 3
  retry_after: 5  # seconds
  
  allowed_fails: 3  # Number of fails before model is removed from routing
  cooldown_time: 60  # seconds before retrying failed model

# General Settings
general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY  # Optional master key for proxy
  database_url: null  # No database for this setup
  
  # Logging
  json_logs: false
  log_responses: false
  
  # Health checks
  health_check_interval: 300  # seconds
  health_check_timeout: 30    # seconds
  
  # CORS
  cors_settings:
    allowed_origins: ["*"]
    allowed_methods: ["GET", "POST", "PUT", "DELETE", "OPTIONS"]
    allowed_headers: ["*"]

# LiteLLM Settings
litellm_settings:
  # Drop unsupported params
  drop_params: true
  
  # Verbosity
  set_verbose: false
  detailed_debug: false
  
  # Timeouts
  request_timeout: 600      # 10 minutes
  stream_timeout: 60        # 1 minute
  max_retry_time: 120       # 2 minutes
  
  # Telemetry
  telemetry: false
  
  # Success callback
  success_callback: []      # Options: ["langfuse", "lunary", "helicone", "wandb"]
  
  # Rate limiting (per model)
  max_parallel_requests: null
  
  # Context window fallbacks
  context_window_fallbacks: [
    {
      "model": "gpt-4",
      "context_window": 8192,
      "fallback_model": "gpt-3.5-turbo-16k"
    }
  ]

# Model-specific rate limits
model_rate_limits:
  - model_name: gpt-4
    rpm: 100        # requests per minute
    tpm: 100000     # tokens per minute
    
  - model_name: gpt-3.5-turbo
    rpm: 500
    tpm: 500000
    
  - model_name: claude-3-opus
    rpm: 50
    tpm: 50000

# Embeddings Models
embedding_models:
  - model_name: text-embedding-ada-002
    litellm_params:
      model: text-embedding-ada-002
      api_key: os.environ/OPENAI_API_KEY
      
  - model_name: voyage-embeddings
    litellm_params:
      model: voyage-01
      api_key: os.environ/VOYAGE_API_KEY
      
  - model_name: cohere-embeddings
    litellm_params:
      model: embed-english-v3.0
      api_key: os.environ/COHERE_API_KEY

# Image Models
image_models:
  - model_name: dall-e-3
    litellm_params:
      model: dall-e-3
      api_key: os.environ/OPENAI_API_KEY
      
  - model_name: stable-diffusion
    litellm_params:
      model: stability-ai/stable-diffusion-xl-1024-v1-0
      api_key: os.environ/STABILITY_API_KEY

# Audio Models
audio_models:
  - model_name: whisper
    litellm_params:
      model: whisper-1
      api_key: os.environ/OPENAI_API_KEY
      
  - model_name: tts-1
    litellm_params:
      model: tts-1
      api_key: os.environ/OPENAI_API_KEY

# Cache Settings (Optional - Redis)
# cache_settings:
#   type: "redis"
#   host: "localhost"
#   port: 6379
#   password: os.environ/REDIS_PASSWORD
#   ttl: 3600  # 1 hour

# Alerting (Optional)
# alerting:
#   webhook_url: os.environ/SLACK_WEBHOOK_URL
#   alert_on:
#     - model_down
#     - high_latency
#     - rate_limit_exceeded

# Custom Headers (Optional)
# custom_headers:
#   X-Custom-Header: "value"
#   Authorization: "Bearer ${CUSTOM_TOKEN}"

# Proxy Authentication (Optional)
# auth:
#   type: "key"  # Options: key, oauth2, jwt
#   required_headers:
#     - "X-API-Key"

# Budget Tracking (Optional)
# budget_settings:
#   max_budget: 1000  # USD
#   budget_duration: "monthly"
#   alert_threshold: 0.8  # Alert at 80% usage