# LiteLLM Models Configuration
# Configure your models here instead of using environment variables

models:
  # Local Models (vLLM, Ollama, etc.)
  # - name: qwen3-72b
  #   model: openai/Qwen/Qwen2.5-72B-Instruct
  #   api_base: http://localhost:8000/v1
  #   api_key: local-key
  
  # - name: qwen3-32b
  #   model: openai/Qwen/Qwen2.5-32B-Instruct
  #   api_base: http://localhost:8001/v1
  #   api_key: local-key
  
  # - name: llama3-70b
  #   model: openai/llama3.1:70b
  #   api_base: http://localhost:11434/v1
  #   api_key: ollama

  # Cloud Providers
  # - name: gpt-4
  #   model: openai/gpt-4
  #   api_base: https://api.openai.com/v1
  #   api_key: ${OPENAI_API_KEY}  # You can use environment variables
  
  # - name: claude-3-opus
  #   model: anthropic/claude-3-opus-20240229
  #   api_base: https://api.anthropic.com/v1
  #   api_key: ${ANTHROPIC_API_KEY}

# Example configuration - uncomment and modify as needed
# models:
#   - name: my-local-model
#     model: openai/model-name
#     api_base: http://localhost:8000/v1
#     api_key: local-key